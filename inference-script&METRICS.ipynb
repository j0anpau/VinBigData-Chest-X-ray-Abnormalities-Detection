{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":75176,"databundleVersionId":8252256,"sourceType":"competition"},{"sourceId":48395,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":40481},{"sourceId":48782,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":40795}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom matplotlib import pyplot as plt\nimport pandas as pd; pd.options.mode.chained_assignment = None\nimport numpy as np\nimport scipy\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.models as models\nimport torch.nn as nn\nimport torch.optim as optim\nfrom PIL import Image","metadata":{"id":"7e05697a","execution":{"iopub.status.busy":"2024-05-24T12:08:19.886072Z","iopub.execute_input":"2024-05-24T12:08:19.886778Z","iopub.status.idle":"2024-05-24T12:08:25.977819Z","shell.execute_reply.started":"2024-05-24T12:08:19.886745Z","shell.execute_reply":"2024-05-24T12:08:25.976910Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#LOAD THE NEW TRAIN AND TEST\n\nDATA_DIR = \"/kaggle/input/amia-public-challenge-2024/\"\nTRAIN_DIR = os.path.join(DATA_DIR, \"train/train\")\nTEST_DIR = os.path.join(DATA_DIR, \"test/test\")\n\n\n# Capture all the relevant full train/test paths\nTRAIN_DICOM_PATHS = [os.path.join(TRAIN_DIR, f_name) for f_name in os.listdir(TRAIN_DIR)]\nTEST_DICOM_PATHS = [os.path.join(TEST_DIR, f_name) for f_name in os.listdir(TEST_DIR)]\nprint(f\"\\n... The number of training files is {len(TRAIN_DICOM_PATHS)} ...\")\nprint(f\"... The number of testing files is {len(TEST_DICOM_PATHS)} ...\")\n\n# Define paths to the relevant csv files\nTRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\n# SS_CSV = os.path.join(DATA_DIR, \"sample_submission.csv\")\n\n# Create the relevant dataframe objects\ntrain_df = pd.read_csv(TRAIN_CSV)\n# ss_df_org = pd.read_csv(SS_CSV)\n\nprint(\"\\n\\nTRAIN DATAFRAME\\n\\n\")\ndisplay(train_df.head(3))\n\n\nTEST_CSV = os.path.join(DATA_DIR, \"test.csv\")\n# SS_CSV = os.path.join(DATA_DIR, \"sample_submission.csv\")\n# Create the relevant dataframe objects\ntest_df = pd.read_csv(TEST_CSV)\n\n\nIMG_SIZE_CSV = os.path.join(DATA_DIR, \"img_size.csv\")\nimg_size_df = pd.read_csv(IMG_SIZE_CSV)\n","metadata":{"id":"b73f6f1b","outputId":"545caea2-3cc0-4837-e865-b6b12daa9452","execution":{"iopub.status.busy":"2024-05-24T12:08:34.080040Z","iopub.execute_input":"2024-05-24T12:08:34.081130Z","iopub.status.idle":"2024-05-24T12:08:34.663014Z","shell.execute_reply.started":"2024-05-24T12:08:34.081097Z","shell.execute_reply":"2024-05-24T12:08:34.662095Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\n... The number of training files is 8573 ...\n... The number of testing files is 6427 ...\n\n\nTRAIN DATAFRAME\n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                           image_id          class_name  class_id rad_id  \\\n0  bM8C97htulC9fHKIDurJHquCXr1KZuug          No finding        14     R5   \n1  0FDQVdLgDKI1sRnPL94LzVh9EvXDVM9m  Aortic enlargement         0    R10   \n2  Dwk2TnGJFaMhyi3OfCrhdZG9ppGglC5w       Consolidation         4     R8   \n\n    x_min  y_min   x_max   y_max  \n0     NaN    NaN     NaN     NaN  \n1  1148.0  503.0  1466.0   823.0  \n2   264.0  732.0   550.0  1119.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>class_name</th>\n      <th>class_id</th>\n      <th>rad_id</th>\n      <th>x_min</th>\n      <th>y_min</th>\n      <th>x_max</th>\n      <th>y_max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>bM8C97htulC9fHKIDurJHquCXr1KZuug</td>\n      <td>No finding</td>\n      <td>14</td>\n      <td>R5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0FDQVdLgDKI1sRnPL94LzVh9EvXDVM9m</td>\n      <td>Aortic enlargement</td>\n      <td>0</td>\n      <td>R10</td>\n      <td>1148.0</td>\n      <td>503.0</td>\n      <td>1466.0</td>\n      <td>823.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dwk2TnGJFaMhyi3OfCrhdZG9ppGglC5w</td>\n      <td>Consolidation</td>\n      <td>4</td>\n      <td>R8</td>\n      <td>264.0</td>\n      <td>732.0</td>\n      <td>550.0</td>\n      <td>1119.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Custom dataset in pytorch","metadata":{"id":"55392d9d"}},{"cell_type":"code","source":"batch_size = 64 # HERE ADJUST IF NEEDED","metadata":{"id":"64e0265e","execution":{"iopub.status.busy":"2024-05-24T12:08:40.754454Z","iopub.execute_input":"2024-05-24T12:08:40.755174Z","iopub.status.idle":"2024-05-24T12:08:40.759037Z","shell.execute_reply.started":"2024-05-24T12:08:40.755139Z","shell.execute_reply":"2024-05-24T12:08:40.758101Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":" # HERE ADJUST IF NEEDED\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    transforms.Grayscale(num_output_channels=3)\n])\n\n# Define a custom dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, dataframe, root_dir, transform=None, train=True):\n        self.dataframe = dataframe\n        self.root_dir = root_dir\n        self.transform = transform\n        self.train = train\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_name = self.dataframe.iloc[idx, 0] + '.png'  # Assuming images are stored as png files\n        img_path = os.path.join(self.root_dir, img_name)\n        image = Image.open(img_path)\n        #convert to color\n        # Convert to RGB if the image is grayscale\n        if image.mode != 'RGB':\n            image = image.convert('RGB')\n        if self.train:\n#             if (self.dataframe.iloc[idx, 2]==14):\n#                 class_id = 0 # normal\n#             else:\n#                 class_id = 1 # lesion\n            class_id = self.dataframe.iloc[idx, 2]  # Class ID\n        else:\n            class_id = 0 # we don't know the label for the test.\n        if self.transform:\n                image = self.transform(image)\n\n        return image, class_id\n","metadata":{"id":"b9a0ca68","execution":{"iopub.status.busy":"2024-05-24T12:08:43.256189Z","iopub.execute_input":"2024-05-24T12:08:43.256538Z","iopub.status.idle":"2024-05-24T12:08:43.266364Z","shell.execute_reply.started":"2024-05-24T12:08:43.256509Z","shell.execute_reply":"2024-05-24T12:08:43.265355Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Create custom datasets and dataloaders\n#train_dataset = CustomDataset(dataframe=train_df, root_dir=TRAIN_DIR, train=True, transform=transform)\n#train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\ntest_dataset = CustomDataset(dataframe=test_df, root_dir=TEST_DIR,train=False,transform=transform)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size,shuffle=False)","metadata":{"id":"0a343172","execution":{"iopub.status.busy":"2024-05-24T12:08:48.012775Z","iopub.execute_input":"2024-05-24T12:08:48.013702Z","iopub.status.idle":"2024-05-24T12:08:48.019679Z","shell.execute_reply.started":"2024-05-24T12:08:48.013659Z","shell.execute_reply":"2024-05-24T12:08:48.018142Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Define model architecture\nmodel = models.resnet18(pretrained=True)\nnum_ftrs = model.fc.in_features\n# model.fc = nn.Linear(num_ftrs, 2)  # Binary classification\nmodel.fc = nn.Linear(num_ftrs, 15)  # Binary classification\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Train the model\nnum_epochs = 5\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"id":"ba316cde","outputId":"637365eb-d9b3-4e68-e1e3-c6987b657650","execution":{"iopub.status.busy":"2024-05-24T12:08:51.717917Z","iopub.execute_input":"2024-05-24T12:08:51.718723Z","iopub.status.idle":"2024-05-24T12:08:52.447585Z","shell.execute_reply.started":"2024-05-24T12:08:51.718692Z","shell.execute_reply":"2024-05-24T12:08:52.446665Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 144MB/s] ","output_type":"stream"},{"name":"stdout","text":"cuda:0\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"'''\n\nmodel.to(device)\nmodel.train()\n\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    for i, (inputs, labels) in enumerate(train_dataloader, 0):  # Unpack data from dataloader\n        inputs, labels = inputs.to(device), labels.to(device)  # Move data to device\n\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if i % 10 == 9:    # Print every 10 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 10))\n            running_loss = 0.0\n\nprint('Finished Training')\n'''","metadata":{"id":"3b1405f0","outputId":"7c5b9781-cc9d-4161-acf2-16e859551224","execution":{"iopub.status.busy":"2024-05-24T12:39:06.026324Z","iopub.execute_input":"2024-05-24T12:39:06.026980Z","iopub.status.idle":"2024-05-24T12:39:06.033042Z","shell.execute_reply.started":"2024-05-24T12:39:06.026950Z","shell.execute_reply":"2024-05-24T12:39:06.032174Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"\"model.to(device)\\nmodel.train()\\n\\nfor epoch in range(num_epochs):\\n    running_loss = 0.0\\n    for i, (inputs, labels) in enumerate(train_dataloader, 0):  # Unpack data from dataloader\\n        inputs, labels = inputs.to(device), labels.to(device)  # Move data to device\\n\\n        optimizer.zero_grad()\\n\\n        outputs = model(inputs)\\n        loss = criterion(outputs, labels)\\n        loss.backward()\\n        optimizer.step()\\n\\n        running_loss += loss.item()\\n        if i % 10 == 9:    # Print every 10 mini-batches\\n            print('[%d, %5d] loss: %.3f' %\\n                  (epoch + 1, i + 1, running_loss / 10))\\n            running_loss = 0.0\\n\\nprint('Finished Training')\\n\""},"metadata":{}}]},{"cell_type":"code","source":"#torch.save(model.state_dict(), 'amia-resnet18_128batch_inference.pth')","metadata":{"id":"35a1d18f","execution":{"iopub.status.busy":"2024-05-15T15:01:00.979622Z","iopub.execute_input":"2024-05-15T15:01:00.980004Z","iopub.status.idle":"2024-05-15T15:01:01.055026Z","shell.execute_reply.started":"2024-05-15T15:01:00.979975Z","shell.execute_reply":"2024-05-15T15:01:01.054075Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#torch.save(model,'amia-resnet18-128b-model.pth')\n","metadata":{"id":"85f0d2c3","execution":{"iopub.status.busy":"2024-05-15T15:01:58.709404Z","iopub.execute_input":"2024-05-15T15:01:58.710342Z","iopub.status.idle":"2024-05-15T15:01:58.793123Z","shell.execute_reply.started":"2024-05-15T15:01:58.710307Z","shell.execute_reply":"2024-05-15T15:01:58.792316Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# HERE PUT THE MODEL WE WANT TO PREDICT\n\nmodel.load_state_dict(torch.load(\"/kaggle/input/resnet18-amia-b128-epoch5/pytorch/resnet18-v2-b64-epoch15/1/amia-resnet18_64batch_15epochs_inference.pth\"))","metadata":{"id":"0f63ba7c","outputId":"5546cc60-cc9c-4839-f42b-8d05ccc1009c","execution":{"iopub.status.busy":"2024-05-24T12:09:32.109879Z","iopub.execute_input":"2024-05-24T12:09:32.110231Z","iopub.status.idle":"2024-05-24T12:09:32.764086Z","shell.execute_reply.started":"2024-05-24T12:09:32.110203Z","shell.execute_reply":"2024-05-24T12:09:32.763152Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-24T12:09:35.102969Z","iopub.execute_input":"2024-05-24T12:09:35.103654Z","iopub.status.idle":"2024-05-24T12:09:35.134173Z","shell.execute_reply.started":"2024-05-24T12:09:35.103614Z","shell.execute_reply":"2024-05-24T12:09:35.133271Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=15, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"import torch.nn.functional as F\n# Inference on the test dataset\nmodel.eval()\ntest_predictions = []\ntest_probabilities = []\nwith torch.no_grad():\n    for i, data in enumerate (test_dataloader):\n        print(\"doing batch\",i)\n        # Unpack the data batch\n        inputs, _ = data  # Assuming data is a tuple (images, labels)\n\n        # Move inputs to the device\n        inputs = inputs.to(device)\n\n        # Move model to the same device as inputs\n        model = model.to(device)\n\n        # Perform inference\n        outputs = model(inputs)\n\n        # Get predictions\n        _, predicted = torch.max(outputs, 1)\n\n        # Get class probabilities\n        probabilities,_ = torch.max(F.softmax(outputs, dim=1),1)\n\n        # Convert tensors to CPU numpy arrays\n        test_predictions.extend(predicted.cpu().numpy())\n        test_probabilities.extend(probabilities.cpu().numpy())\n#         print (test_probabilities)\n\n# Assuming you want to add predictions to the test dataframe\ntest_df['predicted_class'] = test_predictions\ntest_df['probabilities'] = test_probabilities\nprint(test_df.head())\n","metadata":{"id":"67a88b8d","outputId":"676847ee-ea31-4b7f-cefd-e39f0fa71dbb","execution":{"iopub.status.busy":"2024-05-24T12:09:40.485836Z","iopub.execute_input":"2024-05-24T12:09:40.486689Z","iopub.status.idle":"2024-05-24T12:19:49.567109Z","shell.execute_reply.started":"2024-05-24T12:09:40.486634Z","shell.execute_reply":"2024-05-24T12:19:49.566049Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"doing batch 0\ndoing batch 1\ndoing batch 2\ndoing batch 3\ndoing batch 4\ndoing batch 5\ndoing batch 6\ndoing batch 7\ndoing batch 8\ndoing batch 9\ndoing batch 10\ndoing batch 11\ndoing batch 12\ndoing batch 13\ndoing batch 14\ndoing batch 15\ndoing batch 16\ndoing batch 17\ndoing batch 18\ndoing batch 19\ndoing batch 20\ndoing batch 21\ndoing batch 22\ndoing batch 23\ndoing batch 24\ndoing batch 25\ndoing batch 26\ndoing batch 27\ndoing batch 28\ndoing batch 29\ndoing batch 30\ndoing batch 31\ndoing batch 32\ndoing batch 33\ndoing batch 34\ndoing batch 35\ndoing batch 36\ndoing batch 37\ndoing batch 38\ndoing batch 39\ndoing batch 40\ndoing batch 41\ndoing batch 42\ndoing batch 43\ndoing batch 44\ndoing batch 45\ndoing batch 46\ndoing batch 47\ndoing batch 48\ndoing batch 49\ndoing batch 50\ndoing batch 51\ndoing batch 52\ndoing batch 53\ndoing batch 54\ndoing batch 55\ndoing batch 56\ndoing batch 57\ndoing batch 58\ndoing batch 59\ndoing batch 60\ndoing batch 61\ndoing batch 62\ndoing batch 63\ndoing batch 64\ndoing batch 65\ndoing batch 66\ndoing batch 67\ndoing batch 68\ndoing batch 69\ndoing batch 70\ndoing batch 71\ndoing batch 72\ndoing batch 73\ndoing batch 74\ndoing batch 75\ndoing batch 76\ndoing batch 77\ndoing batch 78\ndoing batch 79\ndoing batch 80\ndoing batch 81\ndoing batch 82\ndoing batch 83\ndoing batch 84\ndoing batch 85\ndoing batch 86\ndoing batch 87\ndoing batch 88\ndoing batch 89\ndoing batch 90\ndoing batch 91\ndoing batch 92\ndoing batch 93\ndoing batch 94\ndoing batch 95\ndoing batch 96\ndoing batch 97\ndoing batch 98\ndoing batch 99\ndoing batch 100\ndoing batch 101\ndoing batch 102\ndoing batch 103\ndoing batch 104\ndoing batch 105\ndoing batch 106\ndoing batch 107\ndoing batch 108\ndoing batch 109\ndoing batch 110\ndoing batch 111\ndoing batch 112\ndoing batch 113\ndoing batch 114\ndoing batch 115\ndoing batch 116\ndoing batch 117\ndoing batch 118\ndoing batch 119\ndoing batch 120\ndoing batch 121\ndoing batch 122\ndoing batch 123\ndoing batch 124\ndoing batch 125\ndoing batch 126\ndoing batch 127\ndoing batch 128\ndoing batch 129\ndoing batch 130\ndoing batch 131\ndoing batch 132\ndoing batch 133\ndoing batch 134\ndoing batch 135\ndoing batch 136\ndoing batch 137\ndoing batch 138\ndoing batch 139\ndoing batch 140\ndoing batch 141\ndoing batch 142\ndoing batch 143\ndoing batch 144\ndoing batch 145\ndoing batch 146\ndoing batch 147\ndoing batch 148\ndoing batch 149\ndoing batch 150\ndoing batch 151\ndoing batch 152\ndoing batch 153\ndoing batch 154\ndoing batch 155\ndoing batch 156\ndoing batch 157\ndoing batch 158\ndoing batch 159\ndoing batch 160\ndoing batch 161\ndoing batch 162\ndoing batch 163\ndoing batch 164\ndoing batch 165\ndoing batch 166\ndoing batch 167\ndoing batch 168\ndoing batch 169\ndoing batch 170\ndoing batch 171\ndoing batch 172\ndoing batch 173\ndoing batch 174\ndoing batch 175\ndoing batch 176\ndoing batch 177\ndoing batch 178\ndoing batch 179\ndoing batch 180\ndoing batch 181\ndoing batch 182\ndoing batch 183\ndoing batch 184\ndoing batch 185\ndoing batch 186\ndoing batch 187\ndoing batch 188\ndoing batch 189\ndoing batch 190\ndoing batch 191\ndoing batch 192\ndoing batch 193\ndoing batch 194\ndoing batch 195\ndoing batch 196\ndoing batch 197\ndoing batch 198\ndoing batch 199\ndoing batch 200\ndoing batch 201\ndoing batch 202\ndoing batch 203\ndoing batch 204\ndoing batch 205\ndoing batch 206\ndoing batch 207\ndoing batch 208\ndoing batch 209\ndoing batch 210\ndoing batch 211\ndoing batch 212\ndoing batch 213\ndoing batch 214\ndoing batch 215\ndoing batch 216\ndoing batch 217\ndoing batch 218\ndoing batch 219\ndoing batch 220\ndoing batch 221\ndoing batch 222\ndoing batch 223\ndoing batch 224\ndoing batch 225\ndoing batch 226\ndoing batch 227\ndoing batch 228\ndoing batch 229\ndoing batch 230\ndoing batch 231\ndoing batch 232\ndoing batch 233\ndoing batch 234\ndoing batch 235\ndoing batch 236\ndoing batch 237\ndoing batch 238\ndoing batch 239\ndoing batch 240\ndoing batch 241\ndoing batch 242\ndoing batch 243\ndoing batch 244\ndoing batch 245\ndoing batch 246\ndoing batch 247\ndoing batch 248\ndoing batch 249\ndoing batch 250\ndoing batch 251\ndoing batch 252\ndoing batch 253\ndoing batch 254\ndoing batch 255\ndoing batch 256\ndoing batch 257\ndoing batch 258\ndoing batch 259\ndoing batch 260\ndoing batch 261\ndoing batch 262\ndoing batch 263\ndoing batch 264\ndoing batch 265\ndoing batch 266\ndoing batch 267\ndoing batch 268\ndoing batch 269\ndoing batch 270\ndoing batch 271\ndoing batch 272\ndoing batch 273\ndoing batch 274\ndoing batch 275\ndoing batch 276\ndoing batch 277\ndoing batch 278\ndoing batch 279\ndoing batch 280\ndoing batch 281\ndoing batch 282\ndoing batch 283\ndoing batch 284\ndoing batch 285\ndoing batch 286\ndoing batch 287\ndoing batch 288\ndoing batch 289\ndoing batch 290\ndoing batch 291\ndoing batch 292\ndoing batch 293\ndoing batch 294\ndoing batch 295\ndoing batch 296\ndoing batch 297\ndoing batch 298\ndoing batch 299\ndoing batch 300\ndoing batch 301\ndoing batch 302\ndoing batch 303\ndoing batch 304\ndoing batch 305\ndoing batch 306\ndoing batch 307\ndoing batch 308\ndoing batch 309\ndoing batch 310\ndoing batch 311\ndoing batch 312\ndoing batch 313\ndoing batch 314\ndoing batch 315\ndoing batch 316\ndoing batch 317\ndoing batch 318\ndoing batch 319\ndoing batch 320\ndoing batch 321\ndoing batch 322\ndoing batch 323\ndoing batch 324\ndoing batch 325\ndoing batch 326\ndoing batch 327\ndoing batch 328\ndoing batch 329\ndoing batch 330\ndoing batch 331\ndoing batch 332\ndoing batch 333\ndoing batch 334\ndoing batch 335\ndoing batch 336\ndoing batch 337\ndoing batch 338\ndoing batch 339\ndoing batch 340\ndoing batch 341\ndoing batch 342\ndoing batch 343\n                           image_id  predicted_class  probabilities\n0  3r9OdPSdvQ58qI3VUFUeSKyCvxBpFc0c               13        0.33973\n1  LO2jAm8E96Ih87wJVoqiOXHixrwPMeOm               14        1.00000\n2  PN7S4HbhNp4fht9TTc6DXGOKGkeRTR7W               14        1.00000\n3  l7f2KDvrnrh26v4aYgi0Slj7lVBZMQIL               14        1.00000\n4  if5Pqu95xLUtURzAo72YiSg8GNzJb1F3               14        1.00000\n","output_type":"stream"}]},{"cell_type":"code","source":"test_df['Prob']  = test_df['probabilities'].max()\n    \ntest_df","metadata":{"execution":{"iopub.status.busy":"2024-05-24T12:20:55.304356Z","iopub.execute_input":"2024-05-24T12:20:55.304746Z","iopub.status.idle":"2024-05-24T12:20:55.326180Z","shell.execute_reply.started":"2024-05-24T12:20:55.304716Z","shell.execute_reply":"2024-05-24T12:20:55.325154Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                               image_id  predicted_class  probabilities  Prob\n0      3r9OdPSdvQ58qI3VUFUeSKyCvxBpFc0c               13       0.339730   1.0\n1      LO2jAm8E96Ih87wJVoqiOXHixrwPMeOm               14       1.000000   1.0\n2      PN7S4HbhNp4fht9TTc6DXGOKGkeRTR7W               14       1.000000   1.0\n3      l7f2KDvrnrh26v4aYgi0Slj7lVBZMQIL               14       1.000000   1.0\n4      if5Pqu95xLUtURzAo72YiSg8GNzJb1F3               14       1.000000   1.0\n...                                 ...              ...            ...   ...\n21984  k576EmhRJuLOIBHFyzH7LRcr2JbYFnHM               14       1.000000   1.0\n21985  yFiQoOEOTP6yO3KMmiAQ5zkBjdww7icn               14       0.999976   1.0\n21986  yg7B1t1DO9tMk2uJV0SkqA82y97SPZHa               14       1.000000   1.0\n21987  1oO2FHrNonZqP9i854X6sio2hZj4R4h0                5       0.348019   1.0\n21988  8Q8fPobVc11InzzHAKDfjH2emkfnEdnC               14       1.000000   1.0\n\n[21989 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>predicted_class</th>\n      <th>probabilities</th>\n      <th>Prob</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3r9OdPSdvQ58qI3VUFUeSKyCvxBpFc0c</td>\n      <td>13</td>\n      <td>0.339730</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LO2jAm8E96Ih87wJVoqiOXHixrwPMeOm</td>\n      <td>14</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PN7S4HbhNp4fht9TTc6DXGOKGkeRTR7W</td>\n      <td>14</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>l7f2KDvrnrh26v4aYgi0Slj7lVBZMQIL</td>\n      <td>14</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>if5Pqu95xLUtURzAo72YiSg8GNzJb1F3</td>\n      <td>14</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>21984</th>\n      <td>k576EmhRJuLOIBHFyzH7LRcr2JbYFnHM</td>\n      <td>14</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>21985</th>\n      <td>yFiQoOEOTP6yO3KMmiAQ5zkBjdww7icn</td>\n      <td>14</td>\n      <td>0.999976</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>21986</th>\n      <td>yg7B1t1DO9tMk2uJV0SkqA82y97SPZHa</td>\n      <td>14</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>21987</th>\n      <td>1oO2FHrNonZqP9i854X6sio2hZj4R4h0</td>\n      <td>5</td>\n      <td>0.348019</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>21988</th>\n      <td>8Q8fPobVc11InzzHAKDfjH2emkfnEdnC</td>\n      <td>14</td>\n      <td>1.000000</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>21989 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_df.to_csv('test_predictions.csv', index=False) #SAVE THIS RESULT! ","metadata":{"execution":{"iopub.status.busy":"2024-05-24T12:20:58.848918Z","iopub.execute_input":"2024-05-24T12:20:58.849604Z","iopub.status.idle":"2024-05-24T12:20:58.944994Z","shell.execute_reply.started":"2024-05-24T12:20:58.849572Z","shell.execute_reply":"2024-05-24T12:20:58.943979Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"'''test_df = pd.read_csv('test_predictions.csv')\ntest_df.head()'''\n\n# change 0 for 14\n# change 1 for 0\ntest_df['predicted_class'] = test_df['predicted_class'].replace(0, 14)\ntest_df['predicted_class'] = test_df['predicted_class'].replace(1, 0)\n\ndef prepare_pred(row):\n    prob = str(row['probabilities']) if not pd.isna(row['probabilities']) else '0'  # Convert NaN to '0'\n    if row['predicted_class'] == 14: \n        return '14 ' + prob + ' 0 0 1 1'\n    else:\n        return str(row['predicted_class']) + ' ' + prob + ' 700 700 1000 1000'\n    \n# Apply the function to add text to specific rows\ntest_df['PredictionString'] = test_df.apply(prepare_pred, axis=1)\n\n# # Create a new column by concatenating 'predicted_class' and 'Prob' as strings\n# test_df['PredictionString'] = test_df['predicted_class'].astype(str) + ' ' + test_df['Prob'].astype(str) + ' 0 0 1 1'\n\n# Create a new DataFrame with 'image_id' and 'new_column'\ntest_df_submit = test_df[['image_id', 'PredictionString']]\n\nprint(test_df_submit)\n\ntest_df_submit = test_df_submit.drop_duplicates()\n# duplicate_rows = test_df_submit[test_df_submit.duplicated()]\n\n# if duplicate_rows.empty:\n#     print(\"No duplicate rows found.\")\n# else:\n#     print(\"Duplicate rows found:\")\n#     print(duplicate_rows)\n\n\n#### Part of Submit in the Challenge form ####\n\n\n# Concatenar los resultados de filas con IDs repetidos\ndf_concatenado = test_df_submit.groupby('image_id')['PredictionString'].apply(' '.join).reset_index()\n\n# Guardar el dataframe en un archivo CSV\ndf_concatenado.to_csv('submission.csv', index=False)\n\n\ndf_concatenado\n","metadata":{"execution":{"iopub.status.busy":"2024-05-24T12:25:39.041854Z","iopub.execute_input":"2024-05-24T12:25:39.043359Z","iopub.status.idle":"2024-05-24T12:25:39.829158Z","shell.execute_reply.started":"2024-05-24T12:25:39.043317Z","shell.execute_reply":"2024-05-24T12:25:39.828124Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"                               image_id  \\\n0      3r9OdPSdvQ58qI3VUFUeSKyCvxBpFc0c   \n1      LO2jAm8E96Ih87wJVoqiOXHixrwPMeOm   \n2      PN7S4HbhNp4fht9TTc6DXGOKGkeRTR7W   \n3      l7f2KDvrnrh26v4aYgi0Slj7lVBZMQIL   \n4      if5Pqu95xLUtURzAo72YiSg8GNzJb1F3   \n...                                 ...   \n21984  k576EmhRJuLOIBHFyzH7LRcr2JbYFnHM   \n21985  yFiQoOEOTP6yO3KMmiAQ5zkBjdww7icn   \n21986  yg7B1t1DO9tMk2uJV0SkqA82y97SPZHa   \n21987  1oO2FHrNonZqP9i854X6sio2hZj4R4h0   \n21988  8Q8fPobVc11InzzHAKDfjH2emkfnEdnC   \n\n                              PredictionString  \n0      13 0.3397303819656372 700 700 1000 1000  \n1                               14 1.0 0 0 1 1  \n2                               14 1.0 0 0 1 1  \n3                               14 1.0 0 0 1 1  \n4                               14 1.0 0 0 1 1  \n...                                        ...  \n21984                           14 1.0 0 0 1 1  \n21985            14 0.9999759197235107 0 0 1 1  \n21986                           14 1.0 0 0 1 1  \n21987   5 0.3480185270309448 700 700 1000 1000  \n21988            14 0.9999998807907104 0 0 1 1  \n\n[21989 rows x 2 columns]\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                              image_id                        PredictionString\n0     00X4Pb5TcOhWWwrDwn9UoRDJhwYRuusp                          14 1.0 0 0 1 1\n1     00eCz0yTwisqK7dgZKrdhLh4cMP9FewR                          14 1.0 0 0 1 1\n2     00wsXaGGLhOo977BBHmhbKVNu02fWdPl                          14 1.0 0 0 1 1\n3     02IEFam0BlSztSMY3YeA9svnDJOxTKDg  13 0.306283563375473 700 700 1000 1000\n4     02fQeJYiEhOeebwkwE8wsD0FPyz8EWHD                          14 1.0 0 0 1 1\n...                                ...                                     ...\n6422  zxCwOtAINzbYU681ZHjc8GZvtOz9ErEr                          14 1.0 0 0 1 1\n6423  zxq5d7Jh3j2DTwdFqXMmH1OLUFRweQBE          14 0.44158539175987244 0 0 1 1\n6424  zxxt4VNvrRQHUL58LBI4zDb11JZZ5NKz                          14 1.0 0 0 1 1\n6425  zyD6VqKYEQArknozKmmitQJEjhWqGxZI                          14 1.0 0 0 1 1\n6426  zzbKXfUuvpiGy7LLdoe71idOEz4Ji2Ou                          14 1.0 0 0 1 1\n\n[6427 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>PredictionString</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00X4Pb5TcOhWWwrDwn9UoRDJhwYRuusp</td>\n      <td>14 1.0 0 0 1 1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00eCz0yTwisqK7dgZKrdhLh4cMP9FewR</td>\n      <td>14 1.0 0 0 1 1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00wsXaGGLhOo977BBHmhbKVNu02fWdPl</td>\n      <td>14 1.0 0 0 1 1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>02IEFam0BlSztSMY3YeA9svnDJOxTKDg</td>\n      <td>13 0.306283563375473 700 700 1000 1000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>02fQeJYiEhOeebwkwE8wsD0FPyz8EWHD</td>\n      <td>14 1.0 0 0 1 1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6422</th>\n      <td>zxCwOtAINzbYU681ZHjc8GZvtOz9ErEr</td>\n      <td>14 1.0 0 0 1 1</td>\n    </tr>\n    <tr>\n      <th>6423</th>\n      <td>zxq5d7Jh3j2DTwdFqXMmH1OLUFRweQBE</td>\n      <td>14 0.44158539175987244 0 0 1 1</td>\n    </tr>\n    <tr>\n      <th>6424</th>\n      <td>zxxt4VNvrRQHUL58LBI4zDb11JZZ5NKz</td>\n      <td>14 1.0 0 0 1 1</td>\n    </tr>\n    <tr>\n      <th>6425</th>\n      <td>zyD6VqKYEQArknozKmmitQJEjhWqGxZI</td>\n      <td>14 1.0 0 0 1 1</td>\n    </tr>\n    <tr>\n      <th>6426</th>\n      <td>zzbKXfUuvpiGy7LLdoe71idOEz4Ji2Ou</td>\n      <td>14 1.0 0 0 1 1</td>\n    </tr>\n  </tbody>\n</table>\n<p>6427 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Inference on the test dataset\ntest_predictions = []\ntest_probabilities = []\nall_labels = []\n\nwith torch.no_grad():\n    for i, data in enumerate(test_dataloader):\n        print(\"doing batch\", i)\n        # Unpack the data batch\n        inputs, labels = data  # Assuming data is a tuple (images, labels)\n\n        # Move inputs to the device\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        # Perform inference\n        outputs = model(inputs)\n\n        # Get predictions\n        _, predicted = torch.max(outputs, 1)\n\n        # Get class probabilities\n        probabilities = F.softmax(outputs, dim=1).max(dim=1)[0]\n\n        # Convert tensors to CPU numpy arrays\n        test_predictions.extend(predicted.cpu().numpy())\n        test_probabilities.extend(probabilities.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())","metadata":{"execution":{"iopub.status.busy":"2024-05-24T13:06:02.478359Z","iopub.execute_input":"2024-05-24T13:06:02.479230Z","iopub.status.idle":"2024-05-24T13:14:41.872966Z","shell.execute_reply.started":"2024-05-24T13:06:02.479195Z","shell.execute_reply":"2024-05-24T13:14:41.871670Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"doing batch 0\ndoing batch 1\ndoing batch 2\ndoing batch 3\ndoing batch 4\ndoing batch 5\ndoing batch 6\ndoing batch 7\ndoing batch 8\ndoing batch 9\ndoing batch 10\ndoing batch 11\ndoing batch 12\ndoing batch 13\ndoing batch 14\ndoing batch 15\ndoing batch 16\ndoing batch 17\ndoing batch 18\ndoing batch 19\ndoing batch 20\ndoing batch 21\ndoing batch 22\ndoing batch 23\ndoing batch 24\ndoing batch 25\ndoing batch 26\ndoing batch 27\ndoing batch 28\ndoing batch 29\ndoing batch 30\ndoing batch 31\ndoing batch 32\ndoing batch 33\ndoing batch 34\ndoing batch 35\ndoing batch 36\ndoing batch 37\ndoing batch 38\ndoing batch 39\ndoing batch 40\ndoing batch 41\ndoing batch 42\ndoing batch 43\ndoing batch 44\ndoing batch 45\ndoing batch 46\ndoing batch 47\ndoing batch 48\ndoing batch 49\ndoing batch 50\ndoing batch 51\ndoing batch 52\ndoing batch 53\ndoing batch 54\ndoing batch 55\ndoing batch 56\ndoing batch 57\ndoing batch 58\ndoing batch 59\ndoing batch 60\ndoing batch 61\ndoing batch 62\ndoing batch 63\ndoing batch 64\ndoing batch 65\ndoing batch 66\ndoing batch 67\ndoing batch 68\ndoing batch 69\ndoing batch 70\ndoing batch 71\ndoing batch 72\ndoing batch 73\ndoing batch 74\ndoing batch 75\ndoing batch 76\ndoing batch 77\ndoing batch 78\ndoing batch 79\ndoing batch 80\ndoing batch 81\ndoing batch 82\ndoing batch 83\ndoing batch 84\ndoing batch 85\ndoing batch 86\ndoing batch 87\ndoing batch 88\ndoing batch 89\ndoing batch 90\ndoing batch 91\ndoing batch 92\ndoing batch 93\ndoing batch 94\ndoing batch 95\ndoing batch 96\ndoing batch 97\ndoing batch 98\ndoing batch 99\ndoing batch 100\ndoing batch 101\ndoing batch 102\ndoing batch 103\ndoing batch 104\ndoing batch 105\ndoing batch 106\ndoing batch 107\ndoing batch 108\ndoing batch 109\ndoing batch 110\ndoing batch 111\ndoing batch 112\ndoing batch 113\ndoing batch 114\ndoing batch 115\ndoing batch 116\ndoing batch 117\ndoing batch 118\ndoing batch 119\ndoing batch 120\ndoing batch 121\ndoing batch 122\ndoing batch 123\ndoing batch 124\ndoing batch 125\ndoing batch 126\ndoing batch 127\ndoing batch 128\ndoing batch 129\ndoing batch 130\ndoing batch 131\ndoing batch 132\ndoing batch 133\ndoing batch 134\ndoing batch 135\ndoing batch 136\ndoing batch 137\ndoing batch 138\ndoing batch 139\ndoing batch 140\ndoing batch 141\ndoing batch 142\ndoing batch 143\ndoing batch 144\ndoing batch 145\ndoing batch 146\ndoing batch 147\ndoing batch 148\ndoing batch 149\ndoing batch 150\ndoing batch 151\ndoing batch 152\ndoing batch 153\ndoing batch 154\ndoing batch 155\ndoing batch 156\ndoing batch 157\ndoing batch 158\ndoing batch 159\ndoing batch 160\ndoing batch 161\ndoing batch 162\ndoing batch 163\ndoing batch 164\ndoing batch 165\ndoing batch 166\ndoing batch 167\ndoing batch 168\ndoing batch 169\ndoing batch 170\ndoing batch 171\ndoing batch 172\ndoing batch 173\ndoing batch 174\ndoing batch 175\ndoing batch 176\ndoing batch 177\ndoing batch 178\ndoing batch 179\ndoing batch 180\ndoing batch 181\ndoing batch 182\ndoing batch 183\ndoing batch 184\ndoing batch 185\ndoing batch 186\ndoing batch 187\ndoing batch 188\ndoing batch 189\ndoing batch 190\ndoing batch 191\ndoing batch 192\ndoing batch 193\ndoing batch 194\ndoing batch 195\ndoing batch 196\ndoing batch 197\ndoing batch 198\ndoing batch 199\ndoing batch 200\ndoing batch 201\ndoing batch 202\ndoing batch 203\ndoing batch 204\ndoing batch 205\ndoing batch 206\ndoing batch 207\ndoing batch 208\ndoing batch 209\ndoing batch 210\ndoing batch 211\ndoing batch 212\ndoing batch 213\ndoing batch 214\ndoing batch 215\ndoing batch 216\ndoing batch 217\ndoing batch 218\ndoing batch 219\ndoing batch 220\ndoing batch 221\ndoing batch 222\ndoing batch 223\ndoing batch 224\ndoing batch 225\ndoing batch 226\ndoing batch 227\ndoing batch 228\ndoing batch 229\ndoing batch 230\ndoing batch 231\ndoing batch 232\ndoing batch 233\ndoing batch 234\ndoing batch 235\ndoing batch 236\ndoing batch 237\ndoing batch 238\ndoing batch 239\ndoing batch 240\ndoing batch 241\ndoing batch 242\ndoing batch 243\ndoing batch 244\ndoing batch 245\ndoing batch 246\ndoing batch 247\ndoing batch 248\ndoing batch 249\ndoing batch 250\ndoing batch 251\ndoing batch 252\ndoing batch 253\ndoing batch 254\ndoing batch 255\ndoing batch 256\ndoing batch 257\ndoing batch 258\ndoing batch 259\ndoing batch 260\ndoing batch 261\ndoing batch 262\ndoing batch 263\ndoing batch 264\ndoing batch 265\ndoing batch 266\ndoing batch 267\ndoing batch 268\ndoing batch 269\ndoing batch 270\ndoing batch 271\ndoing batch 272\ndoing batch 273\ndoing batch 274\ndoing batch 275\ndoing batch 276\ndoing batch 277\ndoing batch 278\ndoing batch 279\ndoing batch 280\ndoing batch 281\ndoing batch 282\ndoing batch 283\ndoing batch 284\ndoing batch 285\ndoing batch 286\ndoing batch 287\ndoing batch 288\ndoing batch 289\ndoing batch 290\ndoing batch 291\ndoing batch 292\ndoing batch 293\ndoing batch 294\ndoing batch 295\ndoing batch 296\ndoing batch 297\ndoing batch 298\ndoing batch 299\ndoing batch 300\ndoing batch 301\ndoing batch 302\ndoing batch 303\ndoing batch 304\ndoing batch 305\ndoing batch 306\ndoing batch 307\ndoing batch 308\ndoing batch 309\ndoing batch 310\ndoing batch 311\ndoing batch 312\ndoing batch 313\ndoing batch 314\ndoing batch 315\ndoing batch 316\ndoing batch 317\ndoing batch 318\ndoing batch 319\ndoing batch 320\ndoing batch 321\ndoing batch 322\ndoing batch 323\ndoing batch 324\ndoing batch 325\ndoing batch 326\ndoing batch 327\ndoing batch 328\ndoing batch 329\ndoing batch 330\ndoing batch 331\ndoing batch 332\ndoing batch 333\ndoing batch 334\ndoing batch 335\ndoing batch 336\ndoing batch 337\ndoing batch 338\ndoing batch 339\ndoing batch 340\ndoing batch 341\ndoing batch 342\ndoing batch 343\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m         all_labels\u001b[38;5;241m.\u001b[39mextend(labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Assuming you want to add predictions to the test dataframe\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m test_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_class\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_predictions\n\u001b[1;32m     32\u001b[0m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprobabilities\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_probabilities\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'test.csv'","output_type":"error"}]},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_fscore_support\n","metadata":{"execution":{"iopub.status.busy":"2024-05-24T13:15:32.377233Z","iopub.execute_input":"2024-05-24T13:15:32.378028Z","iopub.status.idle":"2024-05-24T13:15:32.862196Z","shell.execute_reply.started":"2024-05-24T13:15:32.377994Z","shell.execute_reply":"2024-05-24T13:15:32.861420Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Assuming you want to add predictions to the test dataframe\ntest_df = pd.read_csv('/kaggle/input/amia-public-challenge-2024/test.csv')\ntest_df['predicted_class'] = test_predictions\ntest_df['probabilities'] = test_probabilities\nprint(test_df.head())\n\n# Calculate the F1, Precision, and Recall scores for each class\nprecision, recall, f1, _ = precision_recall_fscore_support(all_labels, test_predictions, average=None)\n\nfor i in range(len(precision)):\n    print(f'Class {i}:')\n    print(f'Precision: {precision[i]}')\n    print(f'Recall: {recall[i]}')\n    print(f'F1 Score: {f1[i]}')","metadata":{"execution":{"iopub.status.busy":"2024-05-24T13:15:34.072215Z","iopub.execute_input":"2024-05-24T13:15:34.072927Z","iopub.status.idle":"2024-05-24T13:15:34.199188Z","shell.execute_reply.started":"2024-05-24T13:15:34.072898Z","shell.execute_reply":"2024-05-24T13:15:34.198168Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"                           image_id  predicted_class  probabilities\n0  3r9OdPSdvQ58qI3VUFUeSKyCvxBpFc0c               13        0.33973\n1  LO2jAm8E96Ih87wJVoqiOXHixrwPMeOm               14        1.00000\n2  PN7S4HbhNp4fht9TTc6DXGOKGkeRTR7W               14        1.00000\n3  l7f2KDvrnrh26v4aYgi0Slj7lVBZMQIL               14        1.00000\n4  if5Pqu95xLUtURzAo72YiSg8GNzJb1F3               14        1.00000\nClass 0:\nPrecision: 1.0\nRecall: 0.13906953476738368\nF1 Score: 0.24418093983311373\nClass 1:\nPrecision: 0.0\nRecall: 0.0\nF1 Score: 0.0\nClass 2:\nPrecision: 0.0\nRecall: 0.0\nF1 Score: 0.0\nClass 3:\nPrecision: 0.0\nRecall: 0.0\nF1 Score: 0.0\nClass 4:\nPrecision: 0.0\nRecall: 0.0\nF1 Score: 0.0\nClass 5:\nPrecision: 0.0\nRecall: 0.0\nF1 Score: 0.0\nClass 6:\nPrecision: 0.0\nRecall: 0.0\nF1 Score: 0.0\nClass 7:\nPrecision: 0.0\nRecall: 0.0\nF1 Score: 0.0\nClass 8:\nPrecision: 0.0\nRecall: 0.0\nF1 Score: 0.0\nClass 9:\nPrecision: 0.0\nRecall: 0.0\nF1 Score: 0.0\nClass 10:\nPrecision: 0.0\nRecall: 0.0\nF1 Score: 0.0\nClass 11:\nPrecision: 0.0\nRecall: 0.0\nF1 Score: 0.0\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}